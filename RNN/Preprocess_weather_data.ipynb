{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reads in the data, and saves it in numpy arrays for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through all of the files in the data folder, and extract the desired variable. In this case, that variable is the lattitude and logintude grid of that value. For example, the air temperature data is a grid of temperatures, corresponding to different locations on the globe. From each value, 16 years of data are read in and stored by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder = \"C:/users/zdunkerton/documents/githubdata/hurricane/initial_data\"\n",
    "for dir in os.listdir(folder):\n",
    "    data = []\n",
    "    time = []\n",
    "    for filename in os.listdir(folder+\"/\"+dir):\n",
    "        x = Dataset(folder+\"/\"+dir+\"/\"+filename,'r').variables[filename.split('.')[0]]\n",
    "        y = Dataset(folder+\"/\"+dir+\"/\"+filename,'r').variables[\"time\"]\n",
    "        if(x.shape[0]>1460):\n",
    "            x =x[:1460,:,:]\n",
    "            y = y[:1460]\n",
    "        time.append(y)\n",
    "        data.append(x)\n",
    "    time = np.array(time)\n",
    "    time = np.reshape(time, (23360,))\n",
    "    data = np.array(data)\n",
    "    time_name = \"C:/Users/zdunkerton/Documents/GitHubData/Hurricane/Preprocessed/time.npy\"\n",
    "    data_name = \"./\"+dir+\".npy\"\n",
    "    np.save(time_name, time)\n",
    "    np.save(data_name, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am saving and reloading the variables so I have them, but this isn't necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.load(\"C:/Users/zdunkerton/Documents/GitHubData/Hurricane/Preprocessed/Temp_Surface_4xDaily.npy\")\n",
    "wind = np.load(\"C:/Users/zdunkerton/Documents/GitHubData/Hurricane/Preprocessed/UWind_Surface_4xDaily.npy\")\n",
    "humd = np.load(\"C:/Users/zdunkerton/Documents/GitHubData/Hurricane/Preprocessed/RelativeHumidity_Surface_4xDaily.npy\")\n",
    "press = np.load(\"C:/Users/zdunkerton/Documents/GitHubData/Hurricane/Preprocessed/Pressure_Surface_4xDaily.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data on date at a time. For each data, read in the four values and stack them on top of each other. These layers all correspond to the same point in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23360, 4, 73, 144)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 0\n",
    "data= []\n",
    "while year < temp.shape[0]:\n",
    "    day = 0\n",
    "    while day < temp.shape[1]:\n",
    "        temp1 = temp[year][day]\n",
    "        temp1 = scaler.fit_transform(temp1)\n",
    "        wind1 = wind[year][day]\n",
    "        wind1 = scaler.fit_transform(wind1)\n",
    "        humd1 = humd[year][day]\n",
    "        humd1 = scaler.fit_transform(humd1)\n",
    "        pres1 = press[year][day]\n",
    "        pres1 = scaler.fit_transform(pres1)\n",
    "        the_data = np.array([temp1,wind1,humd1,pres1])\n",
    "        data.append(the_data)\n",
    "        day+=1\n",
    "    year+=1\n",
    "data = np.array(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Preprocessed_data.npy\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.load(\"C:/Users/zdunkerton/Documents/GitHubData/Hurricane/Preprocessed/time.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some days that have multiple hurricanes recorded. It is okay that times are only couted as having one hurricane, because the point is to see if there was a hurricane or not, not to determine size or location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./atlantic_hurricane.csv\")\n",
    "i = 0\n",
    "date1 = datetime.datetime(1800,1,1,0,0)\n",
    "hurricane_times = []\n",
    "while i<len(df):\n",
    "    the_time = df.iloc[i,3]\n",
    "    if (the_time !=0) and (the_time !=600) and (the_time !=1200) and (the_time !=1800):\n",
    "        if the_time <=900:\n",
    "            df.iloc[i,3] = 600\n",
    "        elif (the_time >900) and (the_time<=1500):\n",
    "            df.iloc[i,3] = 1200\n",
    "        elif the_time >1500:\n",
    "            df.iloc[i,3] = 1800\n",
    "    hurricane_time = datetime.datetime(int(str(df.iloc[i,2])[:4]), int(str(df.iloc[i,2])[4:6]),int(str(df.iloc[i,2])[6:8]),int(df.iloc[i,3]/100))\n",
    "    converted_time = (hurricane_time-date1).total_seconds()/3600\n",
    "    hurricane_times.append(converted_time)\n",
    "    i+=1\n",
    "labels = []\n",
    "for t in time:\n",
    "    if t in hurricane_times:\n",
    "        labels.append(1)\n",
    "        count+=1\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"labels.npy\", labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
